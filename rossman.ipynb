{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "from matplotlib.dates import MonthLocator, YearLocator\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.mpl_style', 'default')\n",
    "\n",
    "def print_missing_stats(train, test, store_info):\n",
    "    for data_name, data in {'TRAIN': train, 'TEST': test, 'STORE': store_info}.items():\n",
    "        print(data_name, ' (overall = %d)' % len(data))\n",
    "        for attribute in data.columns:\n",
    "            mask = data[attribute].isnull()\n",
    "            k = len(data[attribute][mask])\n",
    "            print('%5d (%2d%%)' % (k, 100*k/len(data)), 'missing values in ', attribute) \n",
    "        print()\n",
    "\n",
    "def load_data(filename_train, filename_test, filename_store, print_missing=False):\n",
    "\n",
    "    train = pd.read_csv(filename_train, header=0, low_memory=False)\n",
    "    test = pd.read_csv(filename_test, header=0, low_memory=False)\n",
    "    store_info = pd.read_csv(filename_store, header=0, low_memory=False)\n",
    "\n",
    "    train.Date = pd.to_datetime(train.Date)\n",
    "    test.Date = pd.to_datetime(test.Date)\n",
    "    \n",
    "    if print_missing:\n",
    "        print('BEFORE:')\n",
    "        print_missing_stats(train, test, store_info)\n",
    "    \n",
    "    test.Open = test.Open.fillna(1)\n",
    "\n",
    "    store_info.CompetitionDistance = store_info.CompetitionDistance.fillna(0)\n",
    "    store_info.CompetitionOpenSinceMonth = store_info.CompetitionOpenSinceMonth.fillna(0).astype(int)\n",
    "    store_info.CompetitionOpenSinceYear = store_info.CompetitionOpenSinceYear.fillna(0).astype(int)\n",
    "    store_info.Promo2SinceWeek = store_info.Promo2SinceWeek.fillna(0).astype(int)\n",
    "    store_info.Promo2SinceYear = store_info.Promo2SinceYear.fillna(0).astype(int)\n",
    "\n",
    "    promo_intervals = [np.NaN] + list(store_info.PromoInterval.value_counts().index)\n",
    "    store_info.PromoInterval = store_info.PromoInterval.map(lambda x: promo_intervals.index(x))\n",
    "    \n",
    "    if print_missing:\n",
    "        print('AFTER:')\n",
    "        print_missing_stats(train, test, store_info)\n",
    "    return train, test, store_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_all_store_sales(train):\n",
    "    fig, axes = pl.subplots(nrows=7, ncols=1, sharey=True, figsize=(20,100))\n",
    "\n",
    "    open_df = train[train.Open == 1]\n",
    "    for day_of_week in range(1, 8):\n",
    "        custom_df = open_df[open_df.DayOfWeek == day_of_week] \n",
    "        gp_store = custom_df.groupby('Store')\n",
    "\n",
    "        for store, group in gp_store:\n",
    "            axes[day_of_week - 1].plot(group['Date'], group['Sales'], 'v--')\n",
    "\n",
    "        gp_date = custom_df.groupby('Date')\n",
    "\n",
    "        ts_mean = gp_date['Sales'].mean()\n",
    "        ts_median = gp_date['Sales'].median()\n",
    "        ts_mean.plot(style='r-', linewidth=5, ax=axes[day_of_week - 1], label='mean')\n",
    "        ts_median.plot(style='b-', linewidth=5, ax=axes[day_of_week - 1], label='median')\n",
    "\n",
    "\n",
    "        axes[day_of_week - 1].set_title('Day ' + str(day_of_week) + '. number of stores = ' + str(len(gp_store)))\n",
    "        axes[day_of_week - 1].legend()\n",
    "        axes[day_of_week - 1].xaxis.set_major_locator(MonthLocator())\n",
    "        axes[day_of_week - 1].grid(True)\n",
    "    pl.savefig('all_stores_and_median.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def construct_label_name(school_holiday, state_holiday, promo_flag, n_stores):\n",
    "    string_school = 'NO SchoolHoliday. '\n",
    "    string_state = 'NO StateHoliday. '\n",
    "    string_promo = 'NO Promo. '\n",
    "    if school_holiday == 1:\n",
    "        string_school = string_school[3:]\n",
    "    if promo_flag:\n",
    "        string_promo = string_promo[3:]\n",
    "    if state_holiday != '0':\n",
    "        string_state = {'a': 'PublicHoliday. ',\n",
    "                        'b': 'EasterHoliday. ',\n",
    "                        'c':'Christmas. '}[state_holiday]\n",
    "    string_stores = '(' + str(n_stores) + ' stores)'\n",
    "    return string_school + string_state + string_promo + string_stores\n",
    "\n",
    "def plot_mean_decomposition(train):\n",
    "    fig, axes = pl.subplots(nrows=7, ncols=1, sharey=True, figsize=(20,100))\n",
    "\n",
    "    open_df = train[train.Open == 1]\n",
    "    for day_of_week in range(1, 8):\n",
    "        day_df = open_df[open_df.DayOfWeek == day_of_week]\n",
    "        for school_holiday in [0, 1]:\n",
    "            school_df = day_df[day_df.SchoolHoliday == school_holiday]\n",
    "            for state_holiday in ['0', 'a', 'b', 'c']:\n",
    "                state_df = school_df[school_df.StateHoliday == state_holiday]\n",
    "                for promo_flag in [0, 1]:\n",
    "                    custom_df = state_df[state_df.Promo == promo_flag]\n",
    "                    if not custom_df.empty:\n",
    "                        gp_date = custom_df.groupby('Date')\n",
    "                        gp_store = custom_df.groupby('Store')\n",
    "\n",
    "                        ts_mean = gp_date.Sales.mean()\n",
    "                        axes[day_of_week - 1].plot(ts_mean.index, ts_mean, 'v--', \n",
    "                                                   label=construct_label_name(school_holiday, state_holiday, \n",
    "                                                                              promo_flag, len(gp_store)))\n",
    "\n",
    "        custom_df = day_df\n",
    "        gp_date = custom_df.groupby('Date')\n",
    "        gp_store = custom_df.groupby('Store')\n",
    "        ts_mean = gp_date.Sales.mean()\n",
    "        ts_mean.plot(style='r-', linewidth=1.5, ax=axes[day_of_week - 1],\n",
    "                     label='mean (' + str(len(gp_store)) + ' stores)')\n",
    "        axes[day_of_week - 1].set_title('Day ' + str(day_of_week))\n",
    "        axes[day_of_week - 1].legend()\n",
    "        axes[day_of_week - 1].xaxis.set_major_locator(MonthLocator())\n",
    "        axes[day_of_week - 1].grid(True)\n",
    "    \n",
    "    \n",
    "    pl.savefig('median_decomposition.png', format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dates_CV(k):\n",
    "    if k >= 20:\n",
    "        k = 19\n",
    "        print('maximum number of cross-validation folds is 19')\n",
    "    date_range = pd.date_range('2013-01-01', '2015-07-31')\n",
    "    for i in range(k):\n",
    "        temp_date_range = date_range.shift(-i*48)[48*i:]\n",
    "        train_date_range = temp_date_range[:-48]\n",
    "        test_date_range = temp_date_range[-48:] \n",
    "        yield train_date_range, test_date_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_with_store(data, store_info):    \n",
    "    store_features = ['Store', 'StoreType', 'Assortment','CompetitionDistance', \n",
    "                      'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear',\n",
    "                      'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "    data = pd.merge(data, store_info[store_features], on='Store', how='left')\n",
    "    return data\n",
    "\n",
    "def get_dummies_values(series, prefix, values):\n",
    "    new_series = pd.get_dummies(series, prefix=prefix)\n",
    "    for value in values:\n",
    "        column_name = prefix + '_' + str(value)\n",
    "        if column_name not in new_series.columns:\n",
    "            new_series = new_series.join(pd.DataFrame(np.zeros(len(new_series)), index=new_series.index,\n",
    "                                                   columns=[column_name]))\n",
    "    return new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_some_features(data):\n",
    "    data['Day'] = data.Date.map(lambda d: d.day).astype(int)\n",
    "    data['Month'] = data.Date.map(lambda d: d.month).astype(int)\n",
    "    data['Week'] = data.Date.map(lambda d: d.week).astype(int)\n",
    "    data['DayOfYear'] = data.Date.map(lambda d: d.dayofyear).astype(int)\n",
    "    data['Year'] = data.Date.map(lambda d: d.year)\n",
    "   \n",
    "    data['DiffToday'] = ((pd.datetime(2015, 9, 18) - data.Date)\n",
    "                                     / np.timedelta64(1, 'D')).astype(int)\n",
    "    data['DiffNewYear'] = data.Date.map(lambda d: (min(d - pd.datetime(d.year, 1, 1),\n",
    "                                  pd.datetime(d.year + 1, 12, 31) - d) / np.timedelta64(1, 'D')).astype(int))\n",
    "    data['DiffFoolDay'] = data.Date.map(lambda d: ((d - pd.datetime(d.year, 4, 1))\n",
    "                                                                           / np.timedelta64(1, 'D')).astype(int))\n",
    "\n",
    "    data = data.join(get_dummies_values(data.StateHoliday, prefix='StateHoliday', values=['0', 'a', 'b', 'c']))\n",
    "    data = data.join(get_dummies_values(data.Month, prefix='Month', values=np.arange(1, 13)))\n",
    "    data = data.join(get_dummies_values(data.StoreType, prefix='StoreType', values=['a', 'b', 'c', 'd']))\n",
    "    data = data.join(get_dummies_values(data.Assortment, prefix='Assortment', values=['a', 'b', 'c']))\n",
    "    data = data.join(get_dummies_values(data.DayOfWeek, prefix='DayOfWeek', values=np.arange(1, 8)))\n",
    "    data = data.join(get_dummies_values(data.CompetitionOpenSinceMonth, prefix='CompetitionOpenSinceMonth',\n",
    "                                        values=np.arange(1, 13)))\n",
    "    \n",
    "    del data['Date'], data['StateHoliday'], data['Month'],\n",
    "    del data['StoreType'], data['Assortment'], data['DayOfWeek'],\n",
    "    del data['CompetitionOpenSinceMonth']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_train(train_data, store_info, train_features):\n",
    "    train_data = train_data[train_features + ['Sales']]\n",
    "        \n",
    "    train_data = merge_with_store(train_data, store_info)\n",
    "    train_data = construct_some_features(train_data)\n",
    "\n",
    "    train_data_labels = np.array(train_data.Sales)\n",
    "    del train_data['Sales']\n",
    "    \n",
    "    features = list(train_data.columns)\n",
    "    return np.array(train_data), train_data_labels, features\n",
    "\n",
    "\n",
    "def construct_test(data, store_info, train_features):\n",
    "    data = merge_with_store(data[train_features], store_info)\n",
    "    data = construct_some_features(data)\n",
    "    \n",
    "    features = list(data.columns)\n",
    "    return np.array(data), features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main_feature_construction(data, validation, date_ranges=None):\n",
    "    train, test, store_info = data\n",
    "    train = train[train.Open == 1]\n",
    "    \n",
    "    if validation:\n",
    "        date_range_train, date_range_test = date_ranges\n",
    "        train_data = train[train.Date.isin(date_range_train)]\n",
    "        test_data = train[train.Date.isin(date_range_test)]\n",
    "    else:\n",
    "        train_data = train\n",
    "        test_data = test\n",
    "    train_features = ['Store', 'DayOfWeek', 'Date', 'Promo', 'SchoolHoliday', 'StateHoliday']\n",
    "    \n",
    "    train_data, train_data_labels, features_names_train = construct_train(train_data, store_info, train_features)\n",
    "    \n",
    "    if validation:\n",
    "        test_data_labels = np.asarray(test_data.Sales)\n",
    "    else:\n",
    "        test_data_labels = None\n",
    "\n",
    "    test_data, features_names_test = construct_test(test_data, store_info, train_features)\n",
    "    \n",
    "    return train_data, train_data_labels, test_data, test_data_labels, features_names_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_feature_importances(clf, features):\n",
    "    importances = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    for i, k in enumerate(sorted_indices):\n",
    "        print('%2d (feature %2d):' % (i, k), features[k], 'Importance = %.5f' % importances[k])\n",
    "    pl.figure(figsize=(20,10))\n",
    "    pl.title('Feature Importance')\n",
    "    pl.bar(range(len(features)), importances[sorted_indices], \n",
    "           color='r', yerr=std[sorted_indices], align='center')\n",
    "    pl.xticks(range(len(features)), sorted_indices)\n",
    "    pl.xlim([-1, len(features) + 1])\n",
    "#     pl.savefig('feature_importance.png', format='png')\n",
    "\n",
    "def compute_RMSPE(test_labels, predicted_labels):\n",
    "    mask = test_labels.nonzero()\n",
    "    y = test_labels[mask]\n",
    "    y_hat = predicted_labels[mask]\n",
    "    return np.sqrt(np.mean(((y - y_hat)/y)**2))\n",
    "\n",
    "def visualize_RMSPE(test_labels, predicted_labels, step = 0.05):\n",
    "    for i in np.arange(0, 1, step):\n",
    "        pl.figure(figsize=(20,7))\n",
    "        pl.plot(test_labels[i*len(test_labels):(i+step)*len(test_labels)],\n",
    "                'b.', label='real')\n",
    "        pl.plot(predicted_labels[i*len(predicted_labels):(i+step)*len(predicted_labels)],\n",
    "                'r.', label='predicted')\n",
    "        pl.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict_model(train_data, train_data_labels, test_data,\n",
    "                      test_data_labels, validation, feature_names, params=None,\n",
    "                      print_importances=True, print_rmspe=False):\n",
    "    \n",
    "    if params:\n",
    "        clf = RandomForestRegressor(n_jobs=-1, **params)\n",
    "    else:\n",
    "        clf = RandomForestRegressor(n_estimators=30, n_jobs=-1, min_samples_leaf=2,\n",
    "                                    max_features=0.464285714286)\n",
    "    \n",
    "#     clf = ElasticNet(normalize=True)\n",
    "    \n",
    "    clf.fit(train_data, np.log(train_data_labels + 1.))\n",
    "    if print_importances and hasattr(clf, 'feature_importances_'):\n",
    "        visualize_feature_importances(clf, feature_names)\n",
    "    \n",
    "    test_predicted_labels = np.exp(clf.predict(test_data)) - 1\n",
    "    if validation: \n",
    "        rmspe_score = compute_RMSPE(test_data_labels, test_predicted_labels)\n",
    "    else:\n",
    "        rmspe_score = None\n",
    "        \n",
    "    return test_predicted_labels, rmspe_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = load_data('train.csv', 'test.csv', 'store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot_all_store_sales(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_mean_decomposition(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folding 1\n",
      "Train ->  ['2013-01-01 00:00:00', '2015-07-31 00:00:00']\n",
      " Test ->  ['2015-06-14 00:00:00', '2015-07-31 00:00:00']\n",
      "rmspe_score = 0.135428\n",
      "Folding 2\n",
      "Train ->  ['2013-01-01 00:00:00', '2015-06-13 00:00:00']\n",
      " Test ->  ['2015-04-27 00:00:00', '2015-06-13 00:00:00']\n",
      "rmspe_score = 0.125158\n",
      "Folding 3\n",
      "Train ->  ['2013-01-01 00:00:00', '2015-04-26 00:00:00']\n",
      " Test ->  ['2015-03-10 00:00:00', '2015-04-26 00:00:00']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-54717621d367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train -> '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Test -> '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_ranges\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain_feature_construction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_ranges\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_ranges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         test_predicted_labels, rmspe_score = fit_predict_model(train_data, train_data_labels, test_data,\n",
      "\u001b[1;32m<ipython-input-73-83c4c479d802>\u001b[0m in \u001b[0;36mmain_feature_construction\u001b[1;34m(data, validation, date_ranges)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtest_data_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_names_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_names_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-66435b7397c8>\u001b[0m in \u001b[0;36mconstruct_test\u001b[1;34m(data, store_info, train_features)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconstruct_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_with_store\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstore_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_some_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-91a79a481e91>\u001b[0m in \u001b[0;36mconstruct_some_features\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      8\u001b[0m     data['DiffToday'] = ((pd.datetime(2015, 9, 18) - data.Date)\n\u001b[0;32m      9\u001b[0m                                      / np.timedelta64(1, 'D')).astype(int)\n\u001b[1;32m---> 10\u001b[1;33m     data['DiffNewYear'] = data.Date.map(lambda d: (min(d - pd.datetime(d.year, 1, 1),\n\u001b[0m\u001b[0;32m     11\u001b[0m                                   pd.datetime(d.year + 1, 12, 31) - d) / np.timedelta64(1, 'D')).astype(int))\n\u001b[0;32m     12\u001b[0m     data['DiffFoolDay'] = data.Date.map(lambda d: ((d - pd.datetime(d.year, 4, 1))\n",
      "\u001b[1;32m/home/oem/anaconda3/lib/python3.4/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   2014\u001b[0m                                      index=self.index).__finalize__(self)\n\u001b[0;32m   2015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2016\u001b[1;33m             \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2017\u001b[0m             return self._constructor(mapped,\n\u001b[0;32m   2018\u001b[0m                                      index=self.index).__finalize__(self)\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:58435)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-91a79a481e91>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m      9\u001b[0m                                      / np.timedelta64(1, 'D')).astype(int)\n\u001b[0;32m     10\u001b[0m     data['DiffNewYear'] = data.Date.map(lambda d: (min(d - pd.datetime(d.year, 1, 1),\n\u001b[1;32m---> 11\u001b[1;33m                                   pd.datetime(d.year + 1, 12, 31) - d) / np.timedelta64(1, 'D')).astype(int))\n\u001b[0m\u001b[0;32m     12\u001b[0m     data['DiffFoolDay'] = data.Date.map(lambda d: ((d - pd.datetime(d.year, 4, 1))\n\u001b[0;32m     13\u001b[0m                                                                            / np.timedelta64(1, 'D')).astype(int))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation = True\n",
    "\n",
    "if validation:\n",
    "    rmspe_cv = 0. # 58 features -> 0.\n",
    "#     params = {\n",
    "#         'n_estimators': range(30, 51, 5),\n",
    "#         'min_samples_lead': range(1,5),\n",
    "#         'max_features': ['auto', 'sqrt', 'log2'] + list(range(5, 31, 5))\n",
    "#     }\n",
    "    number_of_folds = 3\n",
    "    if number_of_folds == 1:\n",
    "        print_importances = True\n",
    "    else:\n",
    "        print_importances = False\n",
    "    for i, date_ranges in enumerate(get_dates_CV(number_of_folds)):\n",
    "        print('Folding', i+1)\n",
    "        print('Train -> ', [str(date_ranges[0][0]), str(date_ranges[0][-1])])\n",
    "        print(' Test -> ', [str(date_ranges[1][0]), str(date_ranges[1][-1])])\n",
    "        result = main_feature_construction(data, validation, date_ranges=date_ranges)\n",
    "        train_data, train_data_labels, test_data, test_data_labels, features = result\n",
    "        test_predicted_labels, rmspe_score = fit_predict_model(train_data, train_data_labels, test_data,\n",
    "                                                               test_data_labels, validation, feature_names=features,\n",
    "                                                               print_importances=print_importances)\n",
    "        print('rmspe_score = %1.6f' % rmspe_score)\n",
    "        rmspe_cv += rmspe_score\n",
    "    rmspe_cv /= folds\n",
    "    print('Number of features =', len(features))\n",
    "    print('Cross-validated RMSPE = %1.6f on %d folds' % (rmspe_cv, number_of_folds))\n",
    "    \n",
    "else:\n",
    "    print_importances = True\n",
    "    result = main_feature_construction(data, validation)\n",
    "    train_data, train_data_labels, test_data, test_data_labels, features = result\n",
    "    test_predicted_labels, rmspe_score = fit_predict_model(train_data, train_data_labels, test_data,\n",
    "                                                           test_data_labels, validation,\n",
    "                                                           print_importances=print_importances)\n",
    "\n",
    "if not validation:\n",
    "    test = data[1][['Id', 'Open']]\n",
    "    test['Sales'] = test_predicted_labels\n",
    "    test.ix[test.Open == 0, 'Sales'] = 0\n",
    "    test[['Id', 'Sales']].to_csv('prediction.csv', index=False)\n",
    "    print('\\nResult was written to prediction.csv')\n",
    "    del test"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
