{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "from matplotlib.dates import MonthLocator, YearLocator\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "filename_test = 'test.csv'\n",
    "filename_train = 'train.csv'\n",
    "filename_store = 'store.csv'\n",
    "\n",
    "train = pd.read_csv(filename_train, header=0, low_memory=False)\n",
    "\n",
    "test = pd.read_csv(filename_test, header=0, low_memory=False)\n",
    "\n",
    "\n",
    "store_info = pd.read_csv(filename_store, header=0, low_memory=False)\n",
    "\n",
    "train.Date = pd.to_datetime(train.Date)\n",
    "test.Date = pd.to_datetime(test.Date)\n",
    "\n",
    "def print_missing_stats():\n",
    "    for data_name, data in {'TRAIN': train, 'TEST': test, 'STORE': store_info}.items():\n",
    "        print(data_name, ' (overall = %d)' % len(data))\n",
    "        for attribute in data.columns:\n",
    "            mask = data[attribute].isnull()\n",
    "            k = len(data[attribute][mask].tolist())\n",
    "            print('%5d (%2d%%)' % (k, 100*k/len(data)), 'missing values in ', attribute) \n",
    "        print()\n",
    "# print_missing_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(test[test.Open.isnull()])\n",
    "test.ix[test.Open.isnull(), 'Open'] = 1\n",
    "\n",
    "store_info.ix[store_info.CompetitionDistance.isnull(),\n",
    "         'CompetitionDistance'] = 0\n",
    "store_info.ix[store_info.CompetitionOpenSinceMonth.isnull(),\n",
    "         'CompetitionOpenSinceMonth'] = 0\n",
    "store_info.ix[store_info.CompetitionOpenSinceYear.isnull(),\n",
    "         'CompetitionOpenSinceYear'] = 0\n",
    "store_info.ix[store_info.Promo2SinceWeek.isnull(),\n",
    "         'Promo2SinceWeek'] = 0\n",
    "store_info.ix[store_info.Promo2SinceYear.isnull(),\n",
    "         'Promo2SinceYear'] = 0\n",
    "\n",
    "promo_intervals = [np.NaN] + list(store_info.PromoInterval.value_counts().index)\n",
    "store_info.PromoInterval = store_info.PromoInterval.map(lambda x: promo_intervals.index(x))\n",
    "# print_missing_stats()\n",
    "del promo_intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting all sales by day of week and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, axes = pl.subplots(nrows=7, ncols=1, sharey=True, figsize=(20,100))\n",
    "\n",
    "# open_df = train[train.Open == 1]\n",
    "# for day_of_week in range(1, 8):\n",
    "#     custom_df = open_df[open_df.DayOfWeek == day_of_week] \n",
    "#     gp_store = custom_df.groupby('Store')\n",
    "\n",
    "#     for store, group in gp_store:\n",
    "#         axes[day_of_week - 1].plot(group['Date'], group['Sales'], 'v--')\n",
    "\n",
    "#     gp_date = custom_df.groupby('Date')\n",
    "\n",
    "#     ts_mean = gp_date['Sales'].mean()\n",
    "#     ts_median = gp_date['Sales'].median()\n",
    "#     ts_mean.plot(style='r-', linewidth=5, ax=axes[day_of_week - 1], label='mean')\n",
    "#     ts_median.plot(style='b-', linewidth=5, ax=axes[day_of_week - 1], label='median')\n",
    "\n",
    "\n",
    "#     axes[day_of_week - 1].set_title('Day ' + str(day_of_week) + '. number of stores = ' + str(len(gp_store)))\n",
    "#     axes[day_of_week - 1].legend()\n",
    "#     axes[day_of_week - 1].xaxis.set_major_locator(MonthLocator())\n",
    "#     axes[day_of_week - 1].grid(True)\n",
    "# # pl.savefig('all_stores_and_median.png', format='png')\n",
    "# del axes,fig, ts_mean, custom_df, gp_date, gp_store, open_df, ts_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting mean of sales by day of week with different values of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def construct_label_name(school_holiday, state_holiday, promo_flag, n_stores):\n",
    "#     string_school = 'NO SchoolHoliday. '\n",
    "#     string_state = 'NO StateHoliday. '\n",
    "#     string_promo = 'NO Promo. '\n",
    "#     if school_holiday == 1:\n",
    "#         string_school = string_school[3:]\n",
    "#     if promo_flag:\n",
    "#         string_promo = string_promo[3:]\n",
    "#     if state_holiday != '0':\n",
    "#         string_state = {'a': 'PublicHoliday. ',\n",
    "#                         'b': 'EasterHoliday. ',\n",
    "#                         'c':'Christmas. '}[state_holiday]\n",
    "#     string_stores = '(' + str(n_stores) + ' stores)'\n",
    "#     return string_school + string_state + string_promo + string_stores\n",
    "\n",
    "# fig, axes = pl.subplots(nrows=7, ncols=1, sharey=True, figsize=(20,100))\n",
    "\n",
    "# open_df = train[train.Open == 1]\n",
    "# for day_of_week in range(1, 8):\n",
    "#     day_df = open_df[open_df.DayOfWeek == day_of_week]\n",
    "#     for school_holiday in [0, 1]:\n",
    "#         school_df = day_df[day_df.SchoolHoliday == school_holiday]\n",
    "#         for state_holiday in ['0', 'a', 'b', 'c']:\n",
    "#             state_df = school_df[school_df.StateHoliday == state_holiday]\n",
    "#             for promo_flag in [0, 1]:\n",
    "#                 custom_df = state_df[state_df.Promo == promo_flag]\n",
    "#                 if not custom_df.empty:\n",
    "#                     gp_date = custom_df.groupby('Date')\n",
    "#                     gp_store = custom_df.groupby('Store')\n",
    "                \n",
    "#                     ts_mean = gp_date.Sales.mean()\n",
    "#                     axes[day_of_week - 1].plot(ts_mean.index, ts_mean, 'v--', \n",
    "#                                  label=construct_label_name(school_holiday, state_holiday,\n",
    "#                                                             promo_flag, len(gp_store)))\n",
    "    \n",
    "                \n",
    "#     custom_df = day_df\n",
    "#     gp_date = custom_df.groupby('Date')\n",
    "#     gp_store = custom_df.groupby('Store')\n",
    "#     ts_mean = gp_date.Sales.mean()\n",
    "#     ts_mean.plot(style='r-', linewidth=1.5, ax=axes[day_of_week - 1],\n",
    "#                  label='mean (' + str(len(gp_store)) + ' stores)')\n",
    "#     axes[day_of_week - 1].set_title('Day ' + str(day_of_week))\n",
    "#     axes[day_of_week - 1].legend()\n",
    "#     axes[day_of_week - 1].xaxis.set_major_locator(MonthLocator())\n",
    "#     axes[day_of_week - 1].grid(True)\n",
    "    \n",
    "    \n",
    "# # pl.savefig('median_decomposition.png', format='png')\n",
    "# del axes,fig, ts_mean, custom_df, gp_date, gp_store, open_df, day_df, school_df, state_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "date_range_train = pd.date_range('2013-01-01', '2015-06-13')\n",
    "date_range_test = pd.date_range('2015-06-14', '2015-07-31')\n",
    "\n",
    "# date_range_train = pd.date_range('2013-01-01', '2014-07-31').union(pd.date_range('2014-09-18', '2015-07-31'))\n",
    "# date_range_test = pd.date_range('2014-08-01', '2014-09-17')\n",
    "\n",
    "validation = False\n",
    "train = train[train.Open == 1]\n",
    "if validation:\n",
    "    train_date_range = train[train.Date.isin(date_range_train)]\n",
    "else:\n",
    "    train_date_range = train\n",
    "    \n",
    "\n",
    "train_features = ['Store', 'DayOfWeek', 'Date', 'Promo']\n",
    "train_date_range = train_date_range[['Customers'] + train_features + ['Sales']]\n",
    "\n",
    "def merge_with_store(data, store_info):    \n",
    "    store_features = ['Store', 'StoreType', 'Assortment','CompetitionDistance', \n",
    "                      'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear',\n",
    "                      'Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
    "    return pd.merge(data, store_info[store_features], on='Store', how='left')\n",
    "\n",
    "train_date_range = merge_with_store(train_date_range, store_info)\n",
    "\n",
    "train_date_range['Day'] = train_date_range.Date.map(lambda d: d.day)\n",
    "# train_date_range['Month'] = train_date_range.Date.map(lambda d: d.month)\n",
    "# train_date_range['Year'] = train_date_range.Date.map(lambda d: d.year)\n",
    "datetime_train = pd.datetime(2015, 9, 18)\n",
    "train_date_range['DiffToday'] = ((datetime_train - train_date_range.Date)\n",
    "                                 / np.timedelta64(1, 'D')).astype(int)\n",
    "train_date_range['DiffNewYear'] = train_date_range.Date.map(lambda d: (min(d - pd.datetime(d.year, 1, 1),\n",
    "                              pd.datetime(d.year + 1, 12, 31) - d)\n",
    "                          / np.timedelta64(1, 'D')).astype(int))\n",
    "train_date_range['DiffFoolDay'] = train_date_range.Date.map(lambda d: ((d - pd.datetime(d.year, 4, 1))\n",
    "                                                                       / np.timedelta64(1, 'D')).astype(int))\n",
    "del train_date_range['Date']\n",
    "    \n",
    "holiday_encoder, storetype_encoder, assortment_encoder = LabelEncoder(), LabelEncoder(), LabelEncoder()\n",
    "holiday_encoder.fit(['0', 'a', 'b', 'c'])\n",
    "storetype_encoder.fit(['a', 'b', 'c', 'd'])\n",
    "assortment_encoder.fit(['a', 'b', 'c'])\n",
    "    \n",
    "# train_date_range.StateHoliday = holiday_encoder.transform(train_date_range.StateHoliday)\n",
    "train_date_range.StoreType = storetype_encoder.transform(train_date_range.StoreType)\n",
    "train_date_range.Assortment = assortment_encoder.transform(train_date_range.Assortment)\n",
    "\n",
    "data_train_labels = np.array(train_date_range.Sales)\n",
    "del train_date_range['Sales']\n",
    "features = list(train_date_range.columns[1:])\n",
    "print(features)\n",
    "print('Number of features =', len(features))\n",
    "data_train = np.array(train_date_range)\n",
    "del train_date_range\n",
    "print('\\nConstructed data_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def construct_test(data, holiday_encoder, storetype_encoder, assortment_encoder):\n",
    "    temp = merge_with_store(data[[x for x in data.columns if x in train_features]], store_info)\n",
    "#     temp.StateHoliday = holiday_encoder.transform(temp.StateHoliday)\n",
    "    temp.StoreType = storetype_encoder.transform(temp.StoreType)\n",
    "    temp.Assortment = assortment_encoder.transform(temp.Assortment)\n",
    "    temp['Day'] = temp.Date.map(lambda d: d.day)\n",
    "#     temp['Month'] = temp.Date.map(lambda d: d.month)\n",
    "#     temp['Year'] = temp.Date.map(lambda d: d.year)\n",
    "    temp['DiffToday'] = ((pd.to_datetime('2015-09-18') - temp.Date)\n",
    "                                 / np.timedelta64(1, 'D')).astype(int)\n",
    "    temp['DiffNewYear'] = temp.Date.map(lambda d: (min(d - pd.datetime(d.year, 1, 1),\n",
    "                              pd.datetime(d.year + 1, 12, 31) - d)\n",
    "                          / np.timedelta64(1, 'D')).astype(int))\n",
    "    temp['DiffFoolDay'] = temp.Date.map(lambda d: ((d - pd.datetime(d.year, 4, 1))\n",
    "                                                                       / np.timedelta64(1, 'D')).astype(int))\n",
    "    del temp['Date']\n",
    "    print(list(temp.columns))\n",
    "    print('Number of features =', len(temp.columns))\n",
    "    return np.array(temp)\n",
    "\n",
    "if validation:\n",
    "    local_test = train[train_features + ['Sales']]\n",
    "    local_test = local_test[local_test.Date.isin(date_range_test)]\n",
    "    local_test_labels = np.asarray(local_test.Sales)\n",
    "    local_test = construct_test(local_test,\n",
    "                                holiday_encoder, storetype_encoder, assortment_encoder)\n",
    "    print('Constructed local_test')\n",
    "else:\n",
    "    data_test = construct_test(test, holiday_encoder, storetype_encoder, assortment_encoder)\n",
    "    print('Constructed data_test')\n",
    "    \n",
    "# customers = data_train[:, 0]\n",
    "data_train = data_train[:, 1:]\n",
    "# df_customers = train[['Customers', 'Sales']]\n",
    "# df_customers = df_customers.groupby('Customers').Sales.mean()    \n",
    "# alpha = 0.15\n",
    "# compound_train_labels = (1 - alpha) * data_train_labels + alpha * df_customers.loc[customers]\n",
    "# del df_customers, customers\n",
    "compound_train_labels = data_train_labels\n",
    "\n",
    "test = test[['Id', 'Open']]\n",
    "del date_range_test, date_range_train, train, store_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_feature_importances(clf, features):\n",
    "    importances = clf.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "    sorted_indices = np.argsort(importances)[::-1]\n",
    "    for i, k in enumerate(sorted_indices):\n",
    "        print('%2d (feature %2d):' % (i, k), features[k], 'Importance = %.5f' % importances[k])\n",
    "    pl.figure(figsize=(20,10))\n",
    "    pl.title('Feature Importance')\n",
    "    pl.bar(range(len(features)), importances[sorted_indices], \n",
    "           color='r', yerr=2*std[sorted_indices], align='center')\n",
    "    pl.xticks(range(len(features)), sorted_indices)\n",
    "    pl.xlim([-1, len(features) + 1])\n",
    "#     pl.savefig('feature_importance.png', format='png')\n",
    "    \n",
    "def compute_RMSPE(test_labels, predicted_labels):\n",
    "    mask = test_labels.nonzero()\n",
    "    y = test_labels[mask]\n",
    "    y_hat = predicted_labels[mask]\n",
    "    return np.sqrt(np.mean(((y - y_hat)/y)**2))\n",
    "\n",
    "def visualize_RMSPE(test_labels, predicted_labels, step = 0.05):\n",
    "    for i in np.arange(0, 1, step):\n",
    "        pl.figure(figsize=(20,7))\n",
    "        pl.plot(test_labels[i*len(test_labels):(i+step)*len(test_labels)],\n",
    "                'b.', label='real')\n",
    "        pl.plot(predicted_labels[i*len(predicted_labels):(i+step)*len(predicted_labels)],\n",
    "                'r.', label='predicted')\n",
    "        pl.legend()\n",
    "\n",
    "rmspe_s, alphas = [], []\n",
    "min_rmspe, min_alpha = 1., 0.\n",
    "\n",
    "# grid_space = np.asarray(np.exp(np.linspace(2.2, 4.4, 20)), dtype=int)\n",
    "grid_space = [0.47]\n",
    "for alpha in grid_space:\n",
    "    # clf = tree.DecisionTreeRegressor()\n",
    "    clf = RandomForestRegressor(n_estimators=30, n_jobs=-1, min_samples_leaf=3, max_features=0.47) # 0.128  0.164 \n",
    "\n",
    "    clf.fit(data_train, compound_train_labels)\n",
    "\n",
    "    if len(grid_space) == 1:\n",
    "        visualize_feature_importances(clf, features)\n",
    "#         visualize_RMSPE(local_test_labels, predicted_test_labels)\n",
    "    else:\n",
    "        print('trying alpha =', alpha, end='\\t')\n",
    "        \n",
    "    if validation:\n",
    "        predicted_test_labels = clf.predict(local_test)\n",
    "        loss_score = compute_RMSPE(local_test_labels, predicted_test_labels)\n",
    "        print('RMSPE on date_range_test =', loss_score)\n",
    "        if loss_score <= min_rmspe:\n",
    "            min_rmspe = loss_score\n",
    "            min_alpha = alpha\n",
    "        alphas.append(alpha)\n",
    "        rmspe_s.append(loss_score)\n",
    "\n",
    "    elif len(grid_space) == 1:\n",
    "        data_test_labels = clf.predict(data_test)\n",
    "        test['Sales'] = data_test_labels\n",
    "        test.ix[test.Open == 0, 'Sales'] = 0\n",
    "        test[['Id', 'Sales']].to_csv('prediction.csv', index=False)\n",
    "        print('\\nResult was written to prediction.csv')\n",
    "    del clf\n",
    "\n",
    "if len(grid_space) > 1:\n",
    "    print('Min RMSPE =', min_rmspe, 'Min alpha =', min_alpha)\n",
    "    pl.figure(figsize=(20,10))\n",
    "    pl.plot(alphas, rmspe_s, 'r.--')\n",
    "    pl.savefig('grid_parameters.png', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting prediction (mean and median by stores) by day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if not validation:\n",
    "#     fig, axes = pl.subplots(nrows=7, ncols=1, sharey=True, figsize=(20,100))\n",
    "    \n",
    "#     train_open = train[train.Open == 1]\n",
    "#     test_open = test[test.Open == 1]\n",
    "#     for day_of_week in range(1, 8):\n",
    "#         custom_df = train_open[train_open.DayOfWeek == day_of_week]\n",
    "#         gp_date = custom_df.groupby('Date')\n",
    "\n",
    "#         ts_mean = gp_date.Sales.mean()\n",
    "#         axes[day_of_week - 1].plot(ts_mean.index, ts_mean, 'r-', linewidth=2, label='mean')\n",
    "#         ts_median = gp_date.Sales.median()\n",
    "#         axes[day_of_week - 1].plot(ts_median.index, ts_median, 'b-', linewidth=2, label='median')\n",
    "        \n",
    "\n",
    "#         custom_df = test_open[test_open.DayOfWeek == day_of_week]\n",
    "#         gp_date = custom_df.groupby('Date')\n",
    "\n",
    "#         ts_mean = gp_date.Sales.mean()\n",
    "#         axes[day_of_week - 1].plot(ts_mean.index, ts_mean, 'k-', linewidth=2, label='mean predicted')\n",
    "#         ts_median = gp_date.Sales.median()\n",
    "#         axes[day_of_week - 1].plot(ts_median.index, ts_median, 'y-', linewidth=2, label='median predicted')\n",
    "        \n",
    "\n",
    "#         axes[day_of_week - 1].xaxis.set_major_locator(YearLocator())\n",
    "#         axes[day_of_week - 1].xaxis.set_minor_locator(MonthLocator())\n",
    "#         axes[day_of_week - 1].set_title('Day ' + str(day_of_week))\n",
    "#         axes[day_of_week - 1].legend()\n",
    "#         axes[day_of_week - 1].grid(True)\n",
    "\n",
    "#     del fig, axes, test['Sales'], train_open, test_open, custom_df, gp_date, ts_mean, ts_median"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
